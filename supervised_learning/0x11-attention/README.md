# 0x11-attention


## General

    What is the attention mechanism?
    How to apply attention to RNNs
    What is a transformer?
    How to create an encoder-decoder transformer model
    What is GPT?
    What is BERT?
    What is self-supervised learning?
    How to use BERT for specific NLP tasks
    What is SQuAD? GLUE?

## referense 
https://towardsdatascience.com/create-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e
https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452